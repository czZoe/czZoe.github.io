[{"authors":null,"categories":null,"content":"HiðŸ‘‹ I am Zhou Chen (å‘¨æ™¨ in Chinese), a fourth-year Ph.D. student in the Department of Computer Science at the National University of Singapore, under Prof. Wei Tsang Ooi. I am also advised by Prof. Shengdong Zhao and work at Synteraction Lab (formerly NUS-HCI Lab).\nI am broadly interested in Human-LLM interaction and developing wearable intelligent assistants that help users seamlessly process information in their daily lives. My work involves deeply understanding user needs and building tailored solutions to meet them. My current work applies the â€˜intent as actionâ€™ paradigm, where users state high-level goals, allowing the AI assistant to autonomously plan and execute the necessary steps. This paradigm, however, often reduces the transparency of the AIâ€™s execution process for the user. This is especially crucial in constrained scenarios typical of wearable devices (e.g., hands-free interaction, limited user attention), where maintaining user understanding and steer is vital.\nTherefore, my primary focus is on identifying user difficulties stemming from inaccurate mental models during interaction and providing users with on-demand, GenAI-powered corrective guidance to augment their cognition and address this challenge.\n Download my resumÃ©. -- ","date":1720483200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720483200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"HiðŸ‘‹ I am Zhou Chen (å‘¨æ™¨ in Chinese), a fourth-year Ph.D. student in the Department of Computer Science at the National University of Singapore, under Prof. Wei Tsang Ooi. I am also advised by Prof.","tags":null,"title":"Zhou Chen","type":"authors"},{"authors":["Zhou Chen","Zihan Yan","Ashwin Ram","Yue Gu","Can Liu","Yun Huang","Wei Tsang Ooi","Shengdong Zhao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1720483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720483200,"objectID":"e4bda080addde460295f5d98667e180d","permalink":"https://czZoe.github.io/publication/zhou2024glassmail/","publishdate":"2024-07-01T00:00:00Z","relpermalink":"/publication/zhou2024glassmail/","section":"publication","summary":"Optical See-through Head-Mounted Displays (OHMDs) offer new opportunities for completing complex information processing tasks on the go. We introduce GlassMail, a Large Language Models (LLMs)-based wearable assistant on OHMDs for mobile email creation. Our formative study identified two challenges of the LLM-based wearable email assistant (i) achieving efficient and accurate understanding of user intentions, and (ii) ensuring effective information presentation for email processes. Through two empirical studies, we developed a \"Single Turn with Optional Clarification \" approach for accurate user intention recognition and a \"Fade Context with Optional Audio \" mode for effective email processing. An observation study then evaluated GlassMail â€™s feasibility in composing formal and semi-formal emails, supporting the usefulness and effectiveness of GlassMail in simple scenarios and yielding insights into potential future improvements for complex scenarios. We further discuss the design implications for the future development of wearable AI-enabled assistants.","tags":["Wearable Intelligent Assistants"],"title":"GlassMail: Towards Personalised Wearable Assistant for On-the-Go Email Creation on Smart Glasses","type":"publication"},{"authors":["Zhou Chen","Katherine Fennedy","Felicia Fang-Yi Tan","Shengdong Zhao","Yurui Shao"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"5816b51114352f5e458bdebc9a5ba080","permalink":"https://czZoe.github.io/publication/zhou2023not/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/zhou2023not/","section":"publication","summary":"The emergent Optical Head-Mounted Display (OHMD) platform has made mobile reading possible by superimposing digital text onto usersâ€™ view of the environment. However, mobile reading through OHMD needs to be effectively balanced with the userâ€™s environmental awareness. Hence, a series of studies were conducted to explore how text spacing strategies facilitate such balance. Through these studies, it was found that increasing spacing within the text can significantly enhance mobile reading on OHMDs in both simple and complex navigation scenarios and that such benefits mainly come from increasing the inter-line spacing, but not inter-word spacing. Compared with existing positioning strategies, increasing inter-line spacing improves mobile OHMD information reading in terms of reading speed (11.9\\% faster), walking speed (3.7\\% faster), and switching between reading and navigation (106.8\\% more accurate and 33\\% faster).","tags":["Ubiquitous Information Processing"],"title":"Not All Spacings are Created Equal: The Effect of Text Spacings in On-the-go Reading Using Optical See-Through Head-Mounted Displays","type":"publication"}]